---
title: '4 ways CS is not an exact science'
subtitle: '(even though it is)'
category: tech
---

Recently I was browsing LinkedIn, as one does when expected to have at least six months of unemployment ahead (I'm looking for internships in Software Engineering or Data Science/Engineering and Machine Learning! If you're hiring, consider me <i class="fas fa-heart" style='color: #DF2935'></i>), when I saw a post that was a vaccination meme, joking about how people were complaining about the [78% efficacy of the vaccine](https://agenciabrasil.ebc.com.br/en/saude/noticia/2021-01/coronavac-has-78-efficiency-against-covid-19) when they would deploy code that didn't meet that same level of efficacy. The post itself aside (I don't think it was a good post for many reasons, not least of which the fact that it was a meme on LinkedIn), there was a comment from a friend of mine asking what kind of professional would deploy code that didn't have 100% efficacy? IT is an exact science after all, right? His comment got me thinking about, well, the exactness of computer science because, right away, some examples came to mind of when we shouldn't and even *couldn't* expect programs to be 100% "effective".

## 1. Computer Systems tend to be very complex

Ok, this first one is more of an excuse than an actual reason, but for a lot of applications this can be an acceptable excuse: Computer systems are complex enough that a solution that works *some*, or even *most* of the time won't work *all* of the time. This is something that almost any programmer that has ever used an online judge or partaken in programming competitions will know: there are often edge-cases, situations that really stretch the definitions of the expected behaviour of the code. Like asking whether or not the empty string is a palindrome, but (in the real world) on steroids. As I said, though, this is an excuse more than a reason. A good programmer will always keep this in mind, and try to find the edge-cases, test for them with their code, and deal with them, but it doesn't always come naturally. It is easy to assume that if you're going to check GPS data you'll always have a correct response from the GPS module, until you don't.

## 2. There are a lot of moving parts

Though increasingly methaforically, as we're moving more and more towards systems with less *phisically* moving parts, systems tend to have a lot of pieces that interact with each other. This means that things can go wrong sometimes. It can be due to the excuse given above, and a part of the system wasn't completely and meticulously thought through and then you have an edge case that causes an incorrect behaviour that causes the program that *you* are writing to also have weird and incorrect behaviour. This might not exactly be an edge-case, but rather a garbage-in, garbage-out scenario. Ultimately your work will still not be 100% effective.

## 3. You don't know what the user might do

Then you have the biggest moving part of all, the user. Even an well-intentioned user might want to use your program for something it's not intended to be used, or interact with it in a way that you thought no one would ever try to interact with it. So you have to try and predict what a user might want to do, and how they might try to achieve that, and factor that into your design choices. They might provide incorrect inputs, or badly-formatted inputs, that ultimately causes a misbehaviour of the program. This becomes even worse when you have some not-so-well-intentioned users, who will actively be looking for flaws, however small they may be, to exploit.

## 4. Not every problem has a one-size-fits-all solution

Even with all that considered there is still the biggest problem of computer science, which is that not every problem you might want to use a computer to solve has a solution that works correctly (and in a timely manner) 100% of the time. There are problems that are just so complicated that, given a big enough instance, they might simply not be solved before you have absolutely no use for the solution anymore. In those cases you have approximative and heuristic algorithms. Things that will give you the correct solution some of the time, or that will give you a solution that is close enough to the correct solution some of the time. A famous, more "theoretical", example of a problem that can't always be solved in a timely manner is the [Travelling Salesman Problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem), such that in practice you might be forced to give a solution that is exact, but will only work in some of the cases (spinning out into infinity when the instance gets big enough), or where you might give a solution that you *know* won't be the *exact* correct solution, but will be close enough. Another great example of a case where there might not be a one-size-fits-all solution is with Machine Learning models, that are ultimately *optimization* solutions. A classification model will *try* to get as close as possible to 100% accuracy (which is a good measure of effectiveness in that scenario), but not get there. In some cases a good model can achieve sub 90% accuracy, even, and still be considered very good, depending on the problem. The [AlphaFold](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) model developed by DeepMind that is being considered a huge game-changer in protein folding does not, itself, achieve 100% accuracy, getting closer to 92% or 87% depending on how you slice it.

## Conclusion

Although I realize this might not be *exactly* the kind of effectiveness that the original post was talking about, and so wasn't exactly what my friend was talking about either (and ultimately this is a way-too-deep analysis of a response to a meme with a life lesson that would make moms everywhere proud), I thought this could be a nice prompt to remember some of the reasons that testing is so important in software: It happens, when writing new code, or when changing existent code, that errors might show up that the developer might not immediately catch, and so automating this process can play a huge role helping developers make sure that their code is, actually, 100% effective (or at least 100-ish% effective in the cases they intend it to be used!)

Besides all that, as a friend pointed out to me after reading the first draft of this post, there is still the fact that software is built by people, most commonly working on teams where there might be misunderstanding, ambiguous definitions, conflicting modelizations, etc. which is all, in itself, an area of study in CS.

Another thing to be considered is that 100% efficacy isn't always what's most important for software engineers and the companies they work for. In a lot of scenarios it is more important to have something out that works for *most* cases, and then solve when it breaks than to spend exponentially more time and money proving that every thing works exactly correct 100% of the time. This is why even companies that hire the best of the best of software engineers, and spend years developing a new product will still be pushing out updates and bug fixes regularly for years and years after it's release.

And, with that being said, there are actual ways to prove programming effectiveness, which can be broken down into correctness (the code does what it is intended to do) and efficiency (it does it in a timely manner). In general, efficiency proofs can be quite important. But in some industries (and in more theoretical and scientific scenarios), being able to *formally* prove both of this is needed. You can't wait for a critical system on an airplane to fail to understand why that was and fix it afterwards. You need it to be 100% effective.